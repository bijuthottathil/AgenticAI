{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c52d8c-4e21-4994-940b-e5d54b282df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-langchain\n  Downloading databricks_langchain-0.8.1-py3-none-any.whl.metadata (2.7 kB)\nCollecting unitycatalog-langchain\n  Downloading unitycatalog_langchain-0.2.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting databricks-ai-bridge>=0.4.2 (from databricks-langchain)\n  Downloading databricks_ai_bridge-0.8.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting databricks-sdk>=0.65.0 (from databricks-langchain)\n  Downloading databricks_sdk-0.67.0-py3-none-any.whl.metadata (39 kB)\nCollecting databricks-vectorsearch>=0.50 (from databricks-langchain)\n  Downloading databricks_vectorsearch-0.59-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain>=0.3.0 (from databricks-langchain)\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting mlflow>=2.20.1 (from databricks-langchain)\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\nCollecting openai>=1.99.9 (from databricks-langchain)\n  Downloading openai-2.0.0-py3-none-any.whl.metadata (29 kB)\nCollecting pydantic>2.10.0 (from databricks-langchain)\n  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\nCollecting langchain-community>=0.2.0 (from unitycatalog-langchain)\n  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain)\n  Downloading unitycatalog_ai-0.3.2-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: mlflow-skinny>=2.19.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (2.21.3)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (1.5.3)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting tiktoken>=0.8.0 (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.7 kB)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.12/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain) (4.11.0)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.65.0->databricks-langchain) (2.32.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.65.0->databricks-langchain) (2.38.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch>=0.50->databricks-langchain) (5.29.4)\nCollecting deprecation>=2 (from databricks-vectorsearch>=0.50->databricks-langchain)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting langchain-core<1.0.0,>=0.3.72 (from langchain>=0.3.0->databricks-langchain)\n  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain>=0.3.0->databricks-langchain)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nCollecting langsmith>=0.1.17 (from langchain>=0.3.0->databricks-langchain)\n  Downloading langsmith-0.4.31-py3-none-any.whl.metadata (14 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain>=0.3.0->databricks-langchain)\n  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.12/site-packages (from langchain>=0.3.0->databricks-langchain) (6.0.1)\nCollecting requests<3,>=2.28.1 (from databricks-sdk>=0.65.0->databricks-langchain)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.7 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain) (8.2.2)\nCollecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain) (1.26.4)\nCollecting mlflow-skinny>=2.19.0 (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.4.0 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading mlflow_tracing-3.4.0-py3-none-any.whl.metadata (19 kB)\nCollecting Flask<4 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\nCollecting cryptography<46,>=43.0.0 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_aarch64.whl.metadata (5.7 kB)\nCollecting docker<8,>=4.0.0 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting fastmcp<3,>=2.0.0 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading fastmcp-2.12.4-py3-none-any.whl.metadata (19 kB)\nCollecting graphene<4 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow>=2.20.1->databricks-langchain)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (3.8.4)\nRequirement already satisfied: pyarrow<22,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (15.0.2)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (1.4.2)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.20.1->databricks-langchain) (1.13.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.1.37)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (7.0.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.31.1)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.31.1)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (24.1)\nCollecting python-dotenv<2,>=0.19.0 (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.34.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain) (1.9.0)\nCollecting httpx<1,>=0.23.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting jiter<1,>=0.4.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading jiter-0.11.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain) (1.3.0)\nCollecting tqdm>4 (from openai>=1.99.9->databricks-langchain)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>2.10.0->databricks-langchain) (0.7.0)\nCollecting pydantic-core==2.33.2 (from pydantic>2.10.0->databricks-langchain)\n  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.8 kB)\nCollecting typing-extensions (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>2.10.0->databricks-langchain)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain) (1.6.0)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain)\n  Downloading unitycatalog_client-0.3.0-py3-none-any.whl.metadata (7.8 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (18 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (12 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (73 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.20.1->databricks-langchain)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.99.9->databricks-langchain) (3.7)\nRequirement already satisfied: cffi>=1.14 in /databricks/python3/lib/python3.12/site-packages (from cryptography<46,>=43.0.0->mlflow>=2.20.1->databricks-langchain) (1.16.0)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.0->unitycatalog-langchain)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow>=2.20.1->databricks-langchain) (2.2.2)\nCollecting authlib>=1.5.2 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading authlib-1.6.4-py2.py3-none-any.whl.metadata (9.8 kB)\nCollecting cyclopts>=3.0.0 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading cyclopts-3.24.0-py3-none-any.whl.metadata (11 kB)\nCollecting exceptiongroup>=1.2.2 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\nCollecting mcp<2.0.0,>=1.12.4 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading mcp-1.15.0-py3-none-any.whl.metadata (80 kB)\nCollecting openapi-core>=0.19.5 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\nCollecting openapi-pydantic>=0.5.1 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\nCollecting pyperclip>=1.9.0 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading pyperclip-1.11.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting rich>=13.9.4 (from fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting jinja2>=3.1.2 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (2.7 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (4.9)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain) (2.9.0.post0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.99.9->databricks-langchain) (2024.6.2)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.99.9->databricks-langchain)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.99.9->databricks-langchain)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain>=0.3.0->databricks-langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain>=0.3.0->databricks-langchain)\n  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain>=0.3.0->databricks-langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain>=0.3.0->databricks-langchain) (0.23.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (1.4.4)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->databricks-ai-bridge>=0.4.2->databricks-langchain) (2024.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk>=0.65.0->databricks-langchain) (2.0.4)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.20.1->databricks-langchain) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.20.1->databricks-langchain) (2.2.0)\nCollecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain>=0.3.0->databricks-langchain)\n  Downloading greenlet-3.2.4-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken>=0.8.0->databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nRequirement already satisfied: databricks-connect<17.1,>=15.1.0 in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (16.4.2)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.14->cryptography<46,>=43.0.0->mlflow>=2.20.1->databricks-langchain) (2.21)\nCollecting docstring-parser>=0.15 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (1.69.2)\nRequirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (1.71.0)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (1.71.0)\nRequirement already satisfied: py4j==0.10.9.9 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (0.10.9.9)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (75.8.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain) (1.16.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.46.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (3.17.0)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.3.0->databricks-langchain)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nCollecting anyio<5,>=3.5.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\nCollecting jsonschema>=4.20.0 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\nCollecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: isodate in /databricks/python3/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain) (0.7.2)\nCollecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\nCollecting more-itertools (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\nCollecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading openapi_schema_validator-0.6.3-py3-none-any.whl.metadata (5.4 kB)\nCollecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading openapi_spec_validator-0.7.2-py3-none-any.whl.metadata (5.7 kB)\nCollecting parse (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.20.1->databricks-langchain)\n  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.2.18)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (0.52b1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.4.8)\nCollecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\nCollecting markdown-it-py>=2.2.0 (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain) (2.15.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.0->unitycatalog-langchain) (1.0.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (1.14.1)\nCollecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain) (5.0.0)\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.2 kB)\nCollecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nCollecting rfc3339-validator (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.1 kB)\nCollecting docutils (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=2.20.1->databricks-langchain)\n  Downloading docutils-0.22.2-py3-none-any.whl.metadata (15 kB)\nDownloading databricks_langchain-0.8.1-py3-none-any.whl (27 kB)\nDownloading unitycatalog_langchain-0.2.0-py3-none-any.whl (5.4 kB)\nDownloading databricks_ai_bridge-0.8.0-py3-none-any.whl (18 kB)\nDownloading databricks_sdk-0.67.0-py3-none-any.whl (718 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/718.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m718.4/718.4 kB\u001B[0m \u001B[31m20.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.59-py3-none-any.whl (15 kB)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m48.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m96.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow-3.4.0-py3-none-any.whl (26.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/26.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m26.5/26.7 MB\u001B[0m \u001B[31m156.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m121.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m93.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.4.0-py3-none-any.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading openai-2.0.0-py3-none-any.whl (955 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/955.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m955.5/955.5 kB\u001B[0m \u001B[31m46.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.11.9-py3-none-any.whl (444 kB)\nDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m88.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading unitycatalog_ai-0.3.2-py3-none-any.whl (66 kB)\nDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m81.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\nDownloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_aarch64.whl (4.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/4.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/4.2 MB\u001B[0m \u001B[31m89.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading fastmcp-2.12.4-py3-none-any.whl (329 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading jiter-0.11.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (338 kB)\nDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\nDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\nDownloading langsmith-0.4.31-py3-none-any.whl (386 kB)\nDownloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\nDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m117.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m62.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nDownloading unitycatalog_client-0.3.0-py3-none-any.whl (159 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\nDownloading authlib-1.6.4-py2.py3-none-any.whl (243 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading cyclopts-3.24.0-py3-none-any.whl (86 kB)\nDownloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\nDownloading frozenlist-1.7.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (243 kB)\nDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.2.4-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (640 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/641.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m641.0/641.0 kB\u001B[0m \u001B[31m30.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (24 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\nDownloading mcp-1.15.0-py3-none-any.whl (166 kB)\nDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\nDownloading multidict-6.6.4-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (258 kB)\nDownloading openapi_core-0.19.5-py3-none-any.whl (106 kB)\nDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\nDownloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\nDownloading orjson-3.11.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (123 kB)\nDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (226 kB)\nDownloading pyperclip-1.11.0-py3-none-any.whl (11 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading regex-2025.9.18-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (797 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/797.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.5/797.5 kB\u001B[0m \u001B[31m39.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading rich-14.1.0-py3-none-any.whl (243 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\nDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (352 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\nDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\nDownloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\nDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\nDownloading openapi_schema_validator-0.6.3-py3-none-any.whl (8.8 kB)\nDownloading openapi_spec_validator-0.7.2-py3-none-any.whl (39 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading rich_rst-1.3.1-py3-none-any.whl (11 kB)\nDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\nDownloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\nDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\nDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\nDownloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\nDownloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (71 kB)\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nDownloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\nDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\nDownloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (385 kB)\nDownloading docutils-0.22.2-py3-none-any.whl (632 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/632.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m632.7/632.7 kB\u001B[0m \u001B[31m30.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\nInstalling collected packages: pyperclip, parse, typing-extensions, tqdm, tabulate, rpds-py, rfc3339-validator, requests, regex, python-multipart, python-dotenv, propcache, pathable, orjson, opentelemetry-proto, multidict, more-itertools, mdurl, marshmallow, markupsafe, lazy-object-proxy, jsonpointer, jiter, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, docutils, docstring-parser, dnspython, deprecation, blinker, attrs, aiohappyeyeballs, yarl, werkzeug, typing-inspection, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, referencing, pydantic-core, markdown-it-py, Mako, jsonpatch, jinja2, httpcore, graphql-relay, exceptiongroup, email-validator, docker, cryptography, anyio, aiosignal, sse-starlette, rich, pydantic, jsonschema-specifications, jsonschema-path, httpx, graphene, Flask, dataclasses-json, databricks-sdk, authlib, alembic, aiohttp, rich-rst, pydantic-settings, openapi-pydantic, openai, langsmith, jsonschema, aiohttp-retry, unitycatalog-client, openapi-schema-validator, mlflow-tracing, mlflow-skinny, mcp, langchain-core, cyclopts, unitycatalog-ai, openapi-spec-validator, langchain-text-splitters, databricks-vectorsearch, databricks-ai-bridge, openapi-core, langchain, langchain-community, fastmcp, unitycatalog-langchain, mlflow, databricks-langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.11.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.2\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Not uninstalling h11 at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'h11'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.20.1\n    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 42.0.5\n    Not uninstalling cryptography at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'cryptography'. No files were found to uninstall.\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.2.0\n    Not uninstalling anyio at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'anyio'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.8.2\n    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.21.3\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75b9b6-c8ac-4794-92a5-9b116d46b3b5\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.16.5 anyio-4.11.0 attrs-25.3.0 authlib-1.6.4 blinker-1.9.0 cryptography-45.0.7 cyclopts-3.24.0 databricks-ai-bridge-0.8.0 databricks-langchain-0.8.1 databricks-sdk-0.67.0 databricks-vectorsearch-0.59 dataclasses-json-0.6.7 deprecation-2.1.0 dnspython-2.8.0 docker-7.1.0 docstring-parser-0.17.0 docutils-0.22.2 email-validator-2.3.0 exceptiongroup-1.3.0 fastmcp-2.12.4 frozenlist-1.7.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.4 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 itsdangerous-2.2.0 jinja2-3.1.6 jiter-0.11.0 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-path-0.3.4 jsonschema-specifications-2025.9.1 langchain-0.3.27 langchain-community-0.3.30 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.4.31 lazy-object-proxy-1.12.0 markdown-it-py-4.0.0 markupsafe-3.0.3 marshmallow-3.26.1 mcp-1.15.0 mdurl-0.1.2 mlflow-3.4.0 mlflow-skinny-3.4.0 mlflow-tracing-3.4.0 more-itertools-10.8.0 multidict-6.6.4 openai-2.0.0 openapi-core-0.19.5 openapi-pydantic-0.5.1 openapi-schema-validator-0.6.3 openapi-spec-validator-0.7.2 opentelemetry-proto-1.37.0 orjson-3.11.3 parse-1.20.2 pathable-0.4.4 propcache-0.3.2 pydantic-2.11.9 pydantic-core-2.33.2 pydantic-settings-2.11.0 pyperclip-1.11.0 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 regex-2025.9.18 requests-2.32.5 requests-toolbelt-1.0.0 rfc3339-validator-0.1.4 rich-14.1.0 rich-rst-1.3.1 rpds-py-0.27.1 sse-starlette-3.0.2 tabulate-0.9.0 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.1 unitycatalog-ai-0.3.2 unitycatalog-client-0.3.0 unitycatalog-langchain-0.2.0 werkzeug-3.1.1 yarl-1.20.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-langchain unitycatalog-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad449726-2860-498d-b5a5-6bd326b30d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb86dbf9-4042-4857-a2b7-7b297894f752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "-- I created this function to data from Samples.tpch tables . Calculates the total profit margin for a given market segment and year\n",
    "CREATE OR REPLACE FUNCTION vectorcatalog.etl.get_profit_margin(\n",
    "  market_segment STRING,\n",
    "  order_year INT\n",
    ")\n",
    "RETURNS DOUBLE\n",
    "COMMENT 'Calculates the total profit margin.'\n",
    "RETURN (\n",
    "  SELECT\n",
    "    SUM(\n",
    "      l.l_extendedprice * (1 - l.l_discount) - (ps.ps_supplycost * l.l_quantity)\n",
    "    )\n",
    "  FROM\n",
    "    samples.tpch.lineitem AS l\n",
    "  JOIN\n",
    "    samples.tpch.orders AS o ON l.l_orderkey = o.o_orderkey\n",
    "  JOIN\n",
    "    samples.tpch.customer AS c ON o.o_custkey = c.c_custkey\n",
    "  JOIN\n",
    "    samples.tpch.partsupp AS ps\n",
    "      ON l.l_partkey = ps.ps_partkey AND l.l_suppkey = ps.ps_suppkey\n",
    "  WHERE\n",
    "    c.c_mktsegment = market_segment\n",
    "    AND YEAR(o.o_orderdate) = order_year\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478d09b9-561e-480c-af84-b3e77faab984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>total_profit_margin</th></tr></thead><tbody><tr><td>2.14471922674296E10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2.14471922674296E10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "total_profit_margin",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate above function \n",
    "\n",
    "segment = 'BUILDING'\n",
    "year = 1995\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        vectorcatalog.etl.get_profit_margin('{segment}', {year}) AS total_profit_margin\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b342141c-f18b-4dca-ad9a-89900342fbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>c_mktsegment</th><th>total_orders</th></tr></thead><tbody><tr><td>HOUSEHOLD</td><td>1490949</td></tr><tr><td>AUTOMOBILE</td><td>1497982</td></tr><tr><td>BUILDING</td><td>1505153</td></tr><tr><td>MACHINERY</td><td>1499152</td></tr><tr><td>FURNITURE</td><td>1506764</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HOUSEHOLD",
         1490949
        ],
        [
         "AUTOMOBILE",
         1497982
        ],
        [
         "BUILDING",
         1505153
        ],
        [
         "MACHINERY",
         1499152
        ],
        [
         "FURNITURE",
         1506764
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "c_mktsegment",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_orders",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 83
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "c_mktsegment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "select * from  vectorcatalog.etl.segmentby_order_aggregated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c536ce-20e1-4a1a-83e2-9350953e87bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "--Creating new function to get total number of orders in the segment by_order_aggregated\n",
    "CREATE or replace FUNCTION vectorcatalog.etl.get_total_orders()\n",
    "RETURNS BIGINT\n",
    "COMMENT 'Returns the total number of all orders in the segment by_order_aggregated.'\n",
    "RETURN (\n",
    "    SELECT COUNT(*) FROM vectorcatalog.etl.segmentby_order_aggregated\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c16febe-1686-47a4-83ea-7e43ba159320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a_total_orders</th></tr></thead><tbody><tr><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a_total_orders",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate above function \n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        vectorcatalog.etl.get_total_orders() AS a_total_orders\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af309b1-df66-4863-881b-32eb1bea08bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tools loaded: 2\nTool names for agent:\n- vectorcatalog__etl__get_profit_margin\n- vectorcatalog__etl__get_total_orders\n"
     ]
    }
   ],
   "source": [
    "# Wrap the UC Function as a LangChain Tool\n",
    "from databricks_langchain import UCFunctionToolkit\n",
    "\n",
    "# Define the fully qualified names of the functions you want to load\n",
    "function_list = [\n",
    "    \"vectorcatalog.etl.get_profit_margin\",\n",
    "    \"vectorcatalog.etl.get_total_orders\" \n",
    "]\n",
    "\n",
    "# Create a toolkit with the list of function names\n",
    "toolkit = UCFunctionToolkit(function_names=function_list)\n",
    "tools = toolkit.tools\n",
    "\n",
    "# Output to verify\n",
    "print(f\"Number of tools loaded: {len(tools)}\")\n",
    "print(f\"Tool names for agent:\")\n",
    "for tool in tools:\n",
    "    print(f\"- {tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81df5ba6-b398-4f0f-a339-9456f6c990a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/connect/session.py:475: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Running Agent (Tool-Calling: Profit Margin) ---\n\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-b0f42e7b-a050-4744-a99e-eda8007998ae/lib/python3.12/site-packages/unitycatalog/ai/core/databricks.py:600: UserWarning: The following parameters do not have descriptions: market_segment, order_year for the function vectorcatalog.etl.get_profit_margin. Using Unity Catalog functions that do not have parameter descriptions limits the functionality for an LLM to understand how to call your function. To improve tool calling accuracy, provide verbose parameter descriptions that fully explain what the expected usage of the function arguments are.\n  check_function_info(function_info)\n{\"ts\": \"2025-09-30 21:07:15.776\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-09-30T21:07:15.775914834+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1932\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"658\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-09-30 21:07:15.776\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-09-30T21:07:15.775914834+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1932\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"658\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-09-30 21:07:15.776\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.FAILED_PRECONDITION\\n\\tdetails = \\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-09-30T21:07:15.775914834+00:00\\\", grpc_status:9, grpc_message:\\\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1932\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"658\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2349: UserWarning: Spark Connect Session expired on the server. Please generate a new session by detaching and reattaching the compute or by calling DatabricksSession.builder.getOrCreate().\n  warnings.warn(\n/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py:255: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=fdad9fee-e78f-44df-918d-7419a76fd0b3, reason=INACTIVITY_TIMEOUT]. (requestId=eba08265-9010-4de9-9b56-7fb5f2d3fc2d)\", grpc_status:9, created_time:\"2025-09-30T21:07:15.806326409+00:00\"}\"\n>.\n  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\nSession expired. Retrying attempt 1 of 5. Refreshing session and retrying after 1 seconds...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m\nInvoking: `vectorcatalog__etl__get_profit_margin` with `{'market_segment': 'BUILDING', 'order_year': 1995}`\n\n\n\u001B[0m"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/connect/session.py:475: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m{\"format\": \"SCALAR\", \"value\": \"21447192267.4296\"}\u001B[0m\u001B[32;1m\u001B[1;3mThe profit margin for the 'BUILDING' segment in 1995 was 21447192267.4296.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAgent Final Output: The profit margin for the 'BUILDING' segment in 1995 was 21447192267.4296.\n\n--- Running Agent (Tool-Calling: Total Orders) ---\n\n\n\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n\u001B[32;1m\u001B[1;3m\nInvoking: `vectorcatalog__etl__get_total_orders` with `{}`\n\n\n\u001B[0m\u001B[33;1m\u001B[1;3m{\"format\": \"SCALAR\", \"value\": \"5\"}\u001B[0m\u001B[32;1m\u001B[1;3mThe total count of all orders is 5.\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAgent Final Output: The total count of all orders is 5.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-562e04bed0ebc7e04c82917d135760a6\", \"tr-53b5c61405a132a2589e8e38ce4b0dac\"]",
      "text/plain": [
       "[Trace(trace_id=tr-562e04bed0ebc7e04c82917d135760a6), Trace(trace_id=tr-53b5c61405a132a2589e8e38ce4b0dac)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 0. Tool Initialization (Assuming get_total_orders and get_profit_margin are registered) ---\n",
    "# Create and Run the Agent. Now, we define the LangChain agent using a Databricks-served LLM and the UC tool.\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks_langchain.uc_ai import DatabricksFunctionClient, UCFunctionToolkit\n",
    "import mlflow\n",
    "\n",
    "# Initialize the Unity Catalog Function Client (automatically uses notebook/CLI credentials)\n",
    "uc_client = DatabricksFunctionClient() \n",
    "\n",
    "# Define the function names to expose as tools\n",
    "# NOTE: Make sure these names exactly match your registered Unity Catalog functions\n",
    "TOOL_PROFIT_MARGIN = \"vectorcatalog.etl.get_profit_margin\"\n",
    "TOOL_TOTAL_ORDERS = \"vectorcatalog.etl.get_total_orders\"\n",
    "\n",
    "# The tools object contains the function metadata needed by the LLM\n",
    "tools = UCFunctionToolkit(\n",
    "    function_names=[TOOL_PROFIT_MARGIN, TOOL_TOTAL_ORDERS], \n",
    "    client=uc_client\n",
    ").tools\n",
    "\n",
    "\n",
    "# --- 1. Initialize the LLM client ---\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\" \n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "\n",
    "# --- 2. Define the Agent Prompt (UPDATED) ---\n",
    "system_prompt = (\n",
    "    \"You are a Senior Financial Analyst and a Data Query Engine. Your goal is to answer questions about \"\n",
    "    \"profitability and order counts accurately. \\n\\n\"\n",
    "    \"RULES:\\n\"\n",
    "    \"1. For any question related to **profit, margin, or financial performance**, you MUST use the \"\n",
    "    \"`get_profit_margin` tool. This tool requires two parameters: a **market_segment (STRING)**, \"\n",
    "    \"such as 'AUTOMOBILE', and an **order_year (INT)**, such as 1997.\\n\"\n",
    "    \"2. If the user asks to compare profitability across years or segments, you must perform multiple tool calls.\\n\"\n",
    "    \"3. For simple questions about the **total number of orders**, use the `get_total_orders` tool.\\n\"\n",
    "    \"4. If required parameters are missing for a tool call, you must first ask the user to clarify.\\n\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 3. Create the Agent ---\n",
    "mlflow.langchain.autolog() \n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# --- 4. Create the Agent Executor ---\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- 5. Invoke the Agent (Example runs demonstrating both tools) ------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Running Agent (Tool-Calling: Profit Margin) ---\")\n",
    "# This question requires calling the complex function\n",
    "result_profit = agent_executor.invoke({\"input\": \"What was the profit margin for the 'BUILDING' segment in 1995?\"})\n",
    "print(f\"Agent Final Output: {result_profit['output']}\")\n",
    "\n",
    "print(\"\\n--- Running Agent (Tool-Calling: Total Orders) ---\")\n",
    "# This question requires calling the simple function\n",
    "result_orders = agent_executor.invoke({\"input\": \"Tell me the total count of all orders.\"})\n",
    "print(f\"Agent Final Output: {result_orders['output']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9003716152870655,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Agentic AI with databricks-langchain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}